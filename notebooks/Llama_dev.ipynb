{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mini_llms.llama2 import Llama2P, ModelArgs\n",
    "from transformers import LlamaConfig, LlamaModel\n",
    "from dataclasses import dataclass\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LlamaConfig()\n",
    "config.vocab_size = 128\n",
    "config.max_position_embeddings = 2048\n",
    "config.num_attention_heads = 16\n",
    "config.num_key_value_heads = 16 # For MHA   \n",
    "config.num_hidden_layers = 24\n",
    "config.hidden_size = 128\n",
    "config.intermediate_size = 128 * 4\n",
    "config.attention_dropout = 0.0\n",
    "\n",
    "llama2_hf = LlamaModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    vocab_size: int = 128\n",
    "    n_positions: int = 2048\n",
    "    n_head: int = 16 \n",
    "    n_layer: int = 24\n",
    "    n_embd: int = 128\n",
    "    attn_pdrop: int = 0.0\n",
    "    embd_pdrop: int = 0.0\n",
    "    resid_pdrop: float = 0.0\n",
    "    flash_attention: bool = False\n",
    "\n",
    "transformer_config = TransformerConfig()\n",
    "\n",
    "llama2_p = Llama2P(transformer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the weights from the llama2_hf model to the llama2_p model\n",
    "sd_hf = llama2_hf.state_dict()\n",
    "sd_keys_hf = sd_hf.keys()\n",
    "\n",
    "sd = llama2_p.state_dict()\n",
    "sd_keys = sd.keys()\n",
    "sd_keys = [k for k in sd_keys if not k.endswith('mask')]\n",
    "\n",
    "for k_hf, k in zip(sd_keys_hf, sd_keys):\n",
    "    assert sd[k].shape == sd_hf[k_hf].shape\n",
    "    with torch.no_grad():\n",
    "        sd[k].copy_(sd_hf[k_hf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.ones(2, 72) * 2\n",
    "batch[1, -10:] *= 0\n",
    "batch = batch.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = llama2_p(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = llama2_hf(batch,\n",
    "              attention_mask=(batch != 0).long(),\n",
    ").last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2050, -1.7073, -1.2383,  ...,  0.8298,  0.7385, -0.9001],\n",
       "         [ 0.2050, -1.7073, -1.2383,  ...,  0.8298,  0.7385, -0.9001],\n",
       "         [ 0.2050, -1.7073, -1.2383,  ...,  0.8298,  0.7385, -0.9001],\n",
       "         ...,\n",
       "         [ 0.2050, -1.7073, -1.2383,  ...,  0.8298,  0.7385, -0.9001],\n",
       "         [ 0.2050, -1.7073, -1.2383,  ...,  0.8298,  0.7385, -0.9001],\n",
       "         [ 0.2050, -1.7073, -1.2383,  ...,  0.8298,  0.7385, -0.9001]],\n",
       "\n",
       "        [[ 0.2050, -1.7073, -1.2383,  ...,  0.8298,  0.7385, -0.9001],\n",
       "         [ 0.2050, -1.7073, -1.2383,  ...,  0.8298,  0.7385, -0.9001],\n",
       "         [ 0.2050, -1.7073, -1.2383,  ...,  0.8298,  0.7385, -0.9001],\n",
       "         ...,\n",
       "         [ 0.2310, -1.7526, -1.1323,  ...,  0.7353,  0.7956, -1.0362],\n",
       "         [ 0.2310, -1.7526, -1.1323,  ...,  0.7353,  0.7956, -1.0362],\n",
       "         [ 0.2310, -1.7526, -1.1323,  ...,  0.7353,  0.7956, -1.0362]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2050, -1.7073, -1.2383,  ...,  0.8298,  0.7385, -0.9001],\n",
       "         [ 0.2050, -1.7073, -1.2383,  ...,  0.8298,  0.7385, -0.9001],\n",
       "         [ 0.2050, -1.7073, -1.2383,  ...,  0.8298,  0.7385, -0.9001],\n",
       "         ...,\n",
       "         [ 0.2050, -1.7073, -1.2383,  ...,  0.8298,  0.7385, -0.9001],\n",
       "         [ 0.2050, -1.7073, -1.2383,  ...,  0.8298,  0.7385, -0.9001],\n",
       "         [ 0.2050, -1.7073, -1.2383,  ...,  0.8298,  0.7385, -0.9001]],\n",
       "\n",
       "        [[ 0.2050, -1.7073, -1.2383,  ...,  0.8298,  0.7385, -0.9001],\n",
       "         [ 0.2050, -1.7073, -1.2383,  ...,  0.8298,  0.7385, -0.9001],\n",
       "         [ 0.2050, -1.7073, -1.2383,  ...,  0.8298,  0.7385, -0.9001],\n",
       "         ...,\n",
       "         [ 0.2310, -1.7526, -1.1323,  ...,  0.7353,  0.7956, -1.0362],\n",
       "         [ 0.2310, -1.7526, -1.1323,  ...,  0.7353,  0.7956, -1.0362],\n",
       "         [ 0.2310, -1.7526, -1.1323,  ...,  0.7353,  0.7956, -1.0362]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
