{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2 # batch size, sequence length, and number of classes\n",
    "x = torch.randn(B, T, C) # input sequence\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1\n",
    "# We want x[b,t] = mean_{i<=t} x[b,i] for all b,t\n",
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t, C)\n",
    "        xbow[b,t] = torch.mean(xprev, dim=0) # (C,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2\n",
    "wei = torch.tril(torch.ones((T, T))) # (T, T)\n",
    "wei = wei / wei.sum(dim=1, keepdim=True) # (T, T)\n",
    "xbow2 = wei @ x # (T, C) @ (B, T, C) -> (B, T, C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones((T, T))) # (T, T)\n",
    "wei = torch.zeros((T, T)) # (T, T)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # (T, T)\n",
    "wei = F.softmax(wei, dim=-1) # (T, T)\n",
    "xbow3 = wei @ x # (T, C) @ (B, T, C) -> (B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a / a.sum(dim=1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32 # batch size, sequence length, and emb dim\n",
    "x = torch.randn(B, T, C) # input sequence\n",
    "\n",
    "# let's implement a single Head of self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "\n",
    "\n",
    "\n",
    "tril = torch.tril(torch.ones((T, T))) # (T, T)\n",
    "wei = torch.zeros((T, T)) # (T, T)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # (T, T)\n",
    "wei = F.softmax(wei, dim=-1) # (T, T)\n",
    "\n",
    "v = value(x) # (B, T, head_size)\n",
    "\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3164, grad_fn=<VarBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0273)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = [\n",
    "    1.2,\n",
    "    6.5,\n",
    "    12.0\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871.754464398402\n",
      "471.28122924930483\n",
      "41.8593451006471\n",
      "75.8284922354904\n",
      "83.40855450323167\n",
      "561.5798303176996\n",
      "2213.5170424448693\n",
      "42.4471150909322\n",
      "82.24714247519591\n",
      "155.08249939894955\n",
      "238.87639026410386\n",
      "1815.4421458356044\n",
      "149.60404951885167\n",
      "68.29644870284929\n",
      "51.44487638462605\n",
      "85.78657251727243\n",
      "102.9197389742889\n",
      "101.07086503018294\n",
      "149.2374891936262\n",
      "137.81200078275265\n",
      "476.31533572953214\n",
      "313.63629766994995\n",
      "42.28592614208186\n",
      "42.298761032962716\n",
      "52.64022282209242\n",
      "58.53996822573236\n",
      "128.7984217845519\n",
      "258.18927818034996\n",
      "276.792449647354\n",
      "128.28941384112343\n",
      "126.79705846781744\n",
      "81.1470610829842\n",
      "1323.2338005636861\n",
      "150.9131498228845\n",
      "17.24824602849145\n",
      "241.61472662632283\n",
      "2481.106179373472\n",
      "393.7196541019415\n",
      "85.15148844801142\n",
      "1859.9063923681458\n",
      "49.624388183978844\n",
      "44.66376257797107\n",
      "153.4966141097154\n",
      "392.615986809046\n",
      "142.58727766478265\n",
      "88.03559102589642\n",
      "966.9462112080839\n",
      "270.80435402425474\n",
      "1320.1805188457563\n",
      "21.759496992843324\n",
      "145.07688928126015\n",
      "64.46665259612502\n",
      "231.96086163318336\n",
      "29.790848096992697\n",
      "77.78518368775137\n",
      "96.18967442732361\n",
      "1236.7616047214922\n",
      "51.46788291103639\n",
      "29.140363408236265\n",
      "93.61023232094173\n",
      "418.85632541107606\n",
      "70.87770946717752\n",
      "72.43011835025057\n",
      "78.23877633881632\n",
      "599.2745627208031\n",
      "171.04997102983683\n",
      "56.445632354205316\n",
      "344.6708377596099\n",
      "34.77306421291451\n",
      "31.51316128290627\n",
      "50.55835034843535\n",
      "117.80894437896866\n",
      "72.98381720676701\n",
      "39.16448541933638\n",
      "21245.118512817706\n",
      "101.84276522224478\n",
      "48.86256427364942\n",
      "116.74732252428804\n",
      "627.3636282063703\n",
      "174.85150947484573\n",
      "408.618358226661\n",
      "64.95068512423492\n",
      "572.0463098761252\n",
      "48.922897026152874\n",
      "226.21984771704385\n",
      "183.98754777947684\n",
      "80.8519928703737\n",
      "69.93593811010697\n",
      "47.958890735439574\n",
      "269.9127950536733\n",
      "194.03511192465058\n",
      "119.97187668061133\n",
      "146.8104175787811\n",
      "189.47967248579647\n",
      "75.78042316034859\n",
      "76.88737916193315\n",
      "323.26662305529794\n",
      "107.77694617506985\n",
      "133.7631901083851\n",
      "162.85206512142403\n"
     ]
    }
   ],
   "source": [
    "# Compute the best possible return on investment (ROI) for the given quotes list\n",
    "# The amount to invest is 50 dollars\n",
    "# The best possible ROI is 12.0\n",
    "# The dollars can be split in any way among the quotes\n",
    "import random\n",
    "\n",
    "for i in range(100):\n",
    "    random.seed(i)\n",
    "    quotes = [random.random() * 10 for _ in range(100)]\n",
    "    amount = 50\n",
    "    best_roi = 0\n",
    "    for i in range(len(quotes)):\n",
    "        for j in range(i+1, len(quotes)):\n",
    "            roi = quotes[j] / quotes[i]\n",
    "            if roi > best_roi:\n",
    "                best_roi = roi\n",
    "    print(best_roi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
