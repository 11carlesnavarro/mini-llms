# mini-GPT

This repository contains an implementation of the GPT (Generative Pre-trained Transformer) architecture, notebooks following the "Make More Neural Networks!" series, and data for model training.

## Directory Structure
.
├── README.md
├── data
│ ├── input.txt
│ └── names.txt
├── gpt_karpathy
│ ├── bigram.py
│ └── gpt.py
└── notebooks
├── GPT_dev.ipynb
├── build_makemore_mlp.ipynb
├── build_makemore_mlp2.ipynb
├── build_makemore_yay.ipynb
├── makemore_part4_backprop.ipynb
└── makemore_part5_cnn1.ipynb

## Data
Located in the `data` directory, you'll find input data used for model training:
- `input.txt`: Contains Shakespearean text.
- `names.txt`: A dataset of English names.

## GPT Karpathy
Within the `gpt_karpathy` directory, you'll find a from-scratch implementation of the GPT architecture based on Andrej Karpathy's educational video titled "Let's build GPT: from scratch, in code, spelled out".
- `bigram.py`: (Description, if needed)
- `gpt.py`: Contains the main GPT implementation.

## Notebooks
The `notebooks` directory contains Jupyter notebooks primarily based on the "Make More Neural Networks!" series. Some of the key notebooks include:
- `GPT_dev.ipynb`: GPT development and experimentation.
- `build_makemore_mlp.ipynb`: A part of the "Make More Neural Networks!" series.
(Continue with brief descriptions for each notebook if needed.)

## Getting Started
1. **Clone the repository**:
```bash
git clone git@github.com:11carlesnavarro/mini-gpt.git
```
